{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04027af4-79c6-4cbf-9cc8-30260c84991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OTEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        super(OTEnv,self).__init__()\n",
    "        self.newRecord = {'tradePosition':0.0,'deposit':0.0,\n",
    "                          'pL':0.0,'roi':0.0,'runningPL':0.0,'runningRoi':0.0,'totalRealizedPL':0.0,'totalRealizedROI':0.0,\n",
    "                         'daysOpen':0.0,'sequencesOpen':0.0} \n",
    "\n",
    "        self.idx_list = []\n",
    "        self.totalroi = []\n",
    "        self.totalpl = []\n",
    "        self.sqz_open = []\n",
    "        self.tradeCount = 0.0\n",
    "        self.current_step = 0.0\n",
    "        self.historicalWindow = 5\n",
    "        self.totalRealizedPL = 0.0\n",
    "        self.totalRealizedROI = 0.0\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # if 'cuda' in device.type:\n",
    "        #     from google.colab import drive\n",
    "        #     drive.mount('/content/drive',force_remount=False)\n",
    "        #     self.data_directory = '/content/drive/Othercomputers/My MacBook Pro/DRLOT/Data.nosync/SPY/stableBaselines/'\n",
    "        # else:\n",
    "        #     self.data_directory = 'Data.nosync/SPY/stableBaselines/'\n",
    "        # self.data_directory = '/content/drive/Othercomputers/My MacBook Pro/DRLOT/Data.nosync/SPY/stableBaselines/'\n",
    "        self.data_directory = '/datasets/spy_calendar/'\n",
    "        \n",
    "        self.closed = False\n",
    "        \n",
    "        # if 'cuda' in device.type:\n",
    "        self.minmax = MinMax3()\n",
    "        # else:\n",
    "            # self.minmax = MinMax2(self.data_directory,reset=False)\n",
    "\n",
    "        self.min_deposit = self.minmax.minimum[self.minmax.columns.index('deposit_mark')]\n",
    "        self.max_deposit = self.minmax.maximum[self.minmax.columns.index('deposit_mark')]\n",
    "        self.min_tradePosition = 0.0\n",
    "        self.max_tradePosition = 1.0\n",
    "        self.min_pL = -25.0\n",
    "        self.max_pL = 25.0\n",
    "        self.min_Roi = -1.0\n",
    "        self.max_Roi = 10.0\n",
    "        self.min_sqz = 10.0\n",
    "        self.max_sqz = 445.0\n",
    "        self.min_days_open = self.minmax.minimum[self.minmax.columns.index('daysToExpiration_front')]\n",
    "        self.max_days_open = self.minmax.maximum[self.minmax.columns.index('daysToExpiration_front')]\n",
    "\n",
    "        self.files = sorted(glob.glob(self.data_directory+'*'))\n",
    "        self.fileUsage = {x:1 for x in self.files}\n",
    "        self.subFileUsage = {x:{} for x in self.files}\n",
    "        self.width = pd.read_parquet(glob.glob(self.data_directory+'*')[0]).shape[-1] + len(self.newRecord)\n",
    "        self.observation_space = Box(low=-2, high=2, shape=(self.historicalWindow*self.width,))\n",
    "        self.action_space = Discrete(2)   \n",
    "\n",
    "        self.seed()\n",
    "\n",
    "        ## not sure if I should reset in the __init__\n",
    "        self.reset()\n",
    "        \n",
    "    def _next_observation(self,):\n",
    "        self.scaler_trade_record()\n",
    "        \n",
    "        ts_a = pd.DataFrame(self.trade_state_list_scaled).to_numpy(dtype=np.float32)\n",
    "        self.obs = np.concatenate((self.minmax.scale(self.dflocked[0:self.historicalWindow]), ts_a), axis=1).flatten()\n",
    "        \n",
    "        # ts_t = torch.Tensor([[np.float32(v) for k,v in ts.items()] for ts in self.trade_state_list_scaled])\n",
    "        # self.obs = torch.concat((self.minmax.scale(self.dflocked[self.current_step-self.historicalWindow:self.current_step]),ts_t),dim=1)\n",
    "        \n",
    "    def _take_action(self,):\n",
    "        self.reward = 0.0\n",
    "        if self.action == 1:\n",
    "            if self.trade_record['tradePosition'] == 0:\n",
    "                self.trade_record['tradePosition'] = 1\n",
    "                self.tradeCount +=1\n",
    "                self.db4exp = self.dflocked.iloc[self.current_step]['daysToExpiration_front']\n",
    "                self.trade_record['deposit'] = round(random.uniform(self.dflocked.iloc[self.current_step]['bid_back'],self.dflocked.iloc[self.current_step]['ask_back']) - \\\n",
    "                                                     random.uniform(self.dflocked.iloc[self.current_step]['bid_front'],self.dflocked.iloc[self.current_step]['ask_front']),2)\n",
    "                if self.trade_record['deposit'] < 0.01:\n",
    "                    self.trade_record['deposit']=0.01\n",
    "\n",
    "            elif self.trade_record['tradePosition'] == 1:\n",
    "                self.trade_record['sequencesOpen'] += 1\n",
    "                self.trade_record['daysOpen'] = self.db4exp - self.dflocked.iloc[self.current_step]['daysToExpiration_front']\n",
    "                self.trade_record['pL'] = np.float32(self.dflocked.iloc[self.current_step]['deposit_mark'] - self.trade_record['deposit'])\n",
    "                self.trade_record['roi'] = np.float32(self.trade_record['pL']/self.trade_record['deposit'])\n",
    "                self.trade_record['runningPL'] = np.float32(self.trade_record['pL'] + self.trade_record['totalRealizedPL'])\n",
    "                self.trade_record['runningRoi'] = np.float32(self.trade_record['roi'] + self.trade_record['totalRealizedROI'])\n",
    "                \n",
    "                self.reward += np.float32(self.trade_record['pL'])\n",
    "                if self.done[self.current_step] == True:\n",
    "                    self.reward -= 1000\n",
    "        \n",
    "        elif self.action == 0:\n",
    "            if self.trade_record['tradePosition'] == 1:\n",
    "                self.sqz_open.append(self.trade_record['sequencesOpen'])\n",
    "                self.trade_record['tradePosition'] = 0\n",
    "                self.trade_record['pL'] = round(random.uniform(self.dflocked.iloc[self.current_step]['bid_back'],self.dflocked.iloc[self.current_step]['ask_back']) - \\\n",
    "                                                random.uniform(self.dflocked.iloc[self.current_step]['bid_front'],self.dflocked.iloc[self.current_step]['ask_front']),2) - self.trade_record['deposit']                \n",
    "                \n",
    "                self.trade_record['roi'] = self.trade_record['pL']/self.trade_record['deposit']    \n",
    "                self.trade_record['totalRealizedPL'] +=  self.trade_record['pL']                 \n",
    "                self.trade_record['totalRealizedROI'] +=  self.trade_record['roi']                 \n",
    "                self.trade_record['deposit'] = 0\n",
    "\n",
    "                self.db4exp = 0\n",
    "                self.closed = True\n",
    "\n",
    "            elif self.trade_record['tradePosition'] == 0:\n",
    "                pass  \n",
    "            \n",
    "        self.reward += np.float32(self.trade_record['totalRealizedPL'])\n",
    "            \n",
    "    def scaler_trade_record(self,):\n",
    "        self.trade_state_list_scaled =[{key: value for key, value in x.items()} for x in self.trade_state_list] #self.trade_state_list.copy()\n",
    "        for d in self.trade_state_list_scaled:\n",
    "            d['tradePosition'] = (((d['tradePosition'] - self.min_tradePosition) / (self.max_tradePosition - self.min_tradePosition))*2)-1   \n",
    "            d['deposit'] = (((d['deposit'] - self.min_deposit) / (self.max_deposit - self.min_deposit))*2)-1\n",
    "            d['pL'] = (((d['pL'] - self.min_pL) / (self.max_pL - self.min_pL))*2)-1\n",
    "            d['roi'] = (((d['roi'] - self.min_Roi ) / (self.max_Roi - self.min_Roi ))*2)-1\n",
    "            d['runningPL'] = (((d['runningPL'] - self.min_pL) / (self.max_pL - self.min_pL))*2)-1    \n",
    "            d['runningRoi'] = (((d['runningRoi'] - self.min_Roi ) / (self.max_Roi - self.min_Roi ))*2)-1    \n",
    "            d['daysOpen'] = (((d['daysOpen'] - self.min_days_open) / (self.max_days_open - self.min_days_open))*2)-1    \n",
    "            d['sequencesOpen'] = (((d['sequencesOpen'] - self.min_sqz ) / (self.max_sqz  - self.min_sqz ))*2)-1    \n",
    "\n",
    "    def step(self, action):    \n",
    "        self.current_step +=1\n",
    "        self.action = action\n",
    "        \n",
    "        self._take_action()\n",
    "                \n",
    "        self.trade_state_list.append({key: value for key, value in self.trade_record.items()})\n",
    "        self._next_observation()\n",
    "        \n",
    "        # display(pd.DataFrame(self.dflocked.iloc[self.current_step]).T[['WD','HOUR','daysToExpiration_front','bid_front','mark_front','ask_front','bid_back','mark_back','ask_back','deposit_mark','openInterest_front','openInterest_back','delta_front','delta_back']])\n",
    "        # display(pd.DataFrame(self.trade_state_list))\n",
    "        \n",
    "        if self.closed == True:\n",
    "            self.trade_record['pL'] = 0.0\n",
    "            self.trade_record['roi'] = 0.0\n",
    "            self.trade_record['daysOpen'] = 0.0\n",
    "            self.trade_record['sequencesOpen'] = 0.0\n",
    "            self.closed = False\n",
    "        \n",
    "        if self.done[self.current_step] == True:\n",
    "            self.trade_record['runningPL'] = 0.0\n",
    "            self.trade_record['runningRoi'] = 0.0 \n",
    "            self.trade_record['totalRealizedPL'] = 0.0\n",
    "            self.trade_record['totalRealizedROI'] = 0.0\n",
    "            \n",
    "        return self.obs, round(self.reward,4) , self.done[self.current_step], {}\n",
    "\n",
    "    def reset(self,seed=1234,return_info=False): #options=**kwargs\n",
    "        self.reward = 0.0\n",
    "        self.trade_record = {key: value for key, value in self.newRecord.items()} \n",
    "        self.db4exp = 0\n",
    "        self.trade_state_list = deque(maxlen=5)\n",
    "        [self.trade_state_list.append({key: value for key, value in self.newRecord.items()} ) for _ in range(5)]\n",
    "        self.current_step = self.historicalWindow - 1\n",
    "        \n",
    "        if len(self.idx_list) == 0:\n",
    "            p_ = sum([v for k,v in self.fileUsage.items()])\n",
    "            self.file = np.random.choice(list(self.fileUsage.keys()),p=[(v/p_) for k,v in self.fileUsage.items()])\n",
    "            for k,v in self.fileUsage.items():\n",
    "                if k != self.file:\n",
    "                    self.fileUsage[k]+=1\n",
    "            self.df = pd.read_parquet(self.file)\n",
    "            self.idx_list = list(self.df.index.unique())\n",
    "\n",
    "        self.idx = np.random.choice(self.idx_list)\n",
    "        self.dflocked = self.df.loc[self.idx].sort_values(['daysToExpiration_front','HOUR'],ascending=[False,True])  \n",
    "        self.idx_list.remove(self.idx)\n",
    "        \n",
    "        self.scaler_trade_record()\n",
    "        \n",
    "        ts_a = pd.DataFrame(self.trade_state_list_scaled).to_numpy(dtype=np.float32)\n",
    "        self.obs = np.concatenate((self.minmax.scale(self.dflocked[0:self.historicalWindow]), ts_a), axis=1).flatten()\n",
    "        \n",
    "        # ts_t = torch.Tensor([[np.float32(v) for k,v in ts.items()] for ts in self.trade_state_list_scaled])\n",
    "        # self.obs = torch.concat((self.minmax.scale(self.dflocked[0:self.historicalWindow]),ts_t),dim=1)\n",
    "        \n",
    "        self.done = [False for _ in range(self.dflocked.shape[0]-1)]+[True]\n",
    "        self.max_steps = len(self.dflocked)\n",
    "        return self.obs\n",
    "        \n",
    "    def render(self, mode='human', close=False):\n",
    "        if len(self.totalpl)==0: tplmean = np.mean(self.totalpl)\n",
    "        else:tplmean = 0\n",
    "            \n",
    "        if len(self.totalpl)==0: troimean = np.mean(self.totalroi)   \n",
    "        else:troimean = 0\n",
    "            \n",
    "        print(f'Average PL: {round(tplmean,2)}, Average ROI: {round(troimean,2)}, Total Trades {self.tradeCount}')\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def close (self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633a0e5e-22b8-46c8-8733-8e02f286e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'utils' already exists and is not an empty directory.\n",
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Requirement already satisfied: gym-OT in /usr/local/lib/python3.8/dist-packages (1.0.3)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (from gym-OT) (0.23.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-OT) (2.2.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym->gym-OT) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-OT) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym->gym-OT) (1.23.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym->gym-OT) (3.10.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jeroencvlier/utils.git\n",
    "!pip install -U -i https://test.pypi.org/simple/ gym-OT\n",
    "import gym_OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d56caf1-fa96-4f8f-90d5-3dfcd45e5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "824026ec-7d80-421e-b1ad-63426b8ef38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"OT-v0\")\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "import gym_OT\n",
    "# select_env = \"OT-v0\"\n",
    "# register_env(select_env, lambda config: OTGym())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e897d2a1-992e-4591-93f6-2bffc3e96bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 20:29:24,174\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.1.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.10', ray_version='2.1.0', ray_commit='be49bde7ee4f6adb3f8710aee0665c27f9f0bb62', address_info={'node_ip_address': '10.42.32.217', 'raylet_ip_address': '10.42.32.217', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-11-17_20-29-22_014487_7760/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-11-17_20-29-22_014487_7760/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-11-17_20-29-22_014487_7760', 'metrics_export_port': 56052, 'gcs_address': '10.42.32.217:61596', 'address': '10.42.32.217:61596', 'dashboard_agent_listen_port': 52365, 'node_id': 'a5a3b6f0c373622fc61845c73c3396d9203a3b2c1b419c751bb4bfd9'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=8,num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2fa293c-fd9d-4606-9133-c7080a789a5d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "TensorFlow was specified as the 'framework' inside of your config dictionary. However, there was no installation found. You can install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ppo\n\u001b[0;32m----> 4\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOTEnv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(algo\u001b[38;5;241m.\u001b[39mtrain())\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:414\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     }\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:445\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_id\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Validate the framework settings in config.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_framework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# Set Algorithm's seed after we have - if necessary - enabled\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# tf eager-execution.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m update_global_seed_if_necessary(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:2311\u001b[0m, in \u001b[0;36mAlgorithm.validate_framework\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m   2302\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m _tf1 \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2303\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour framework setting is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, meaning you are using \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2305\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic-graph mode. Set framework=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to enable eager \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed as with static-graph mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2309\u001b[0m         )\n\u001b[0;32m-> 2311\u001b[0m \u001b[43mcheck_if_correct_nn_framework_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2312\u001b[0m resolve_tf_settings()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:2260\u001b[0m, in \u001b[0;36mAlgorithm.validate_framework.<locals>.check_if_correct_nn_framework_installed\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01min\u001b[39;00m tf_valid_frameworks:\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (_tf1 \u001b[38;5;129;01mor\u001b[39;00m _tf):\n\u001b[0;32m-> 2260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2261\u001b[0m             (\n\u001b[1;32m   2262\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow was specified as the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2263\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minside of your config dictionary. However, there was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2264\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno installation found. You can install TensorFlow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvia `pip install tensorflow`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m             )\n\u001b[1;32m   2267\u001b[0m         )\n\u001b[1;32m   2268\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _torch:\n",
      "\u001b[0;31mImportError\u001b[0m: TensorFlow was specified as the 'framework' inside of your config dictionary. However, there was no installation found. You can install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "algo = ppo.PPO(env=OTEnv)\n",
    "\n",
    "while True:\n",
    "    print(algo.train())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c48b3bd-afd0-471e-a2cb-bbcf0eabb48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn import DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c66017f-02a7-4d00-b60d-9ad473ff4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.get_default_config()['framework']=\"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7b7761c2-aea4-4f9a-aaad-88bdaa1bea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81dd1e05-770f-4707-96cc-8f5336fffc08",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframework\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:414\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     }\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:524\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:185\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    182\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRolloutWorker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Initially, policy_specs will be inferred from config dict.\u001b[39;49;00m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:892\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, policy_specs, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     extra_python_environs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_python_environs_for_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 892\u001b[0m worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_mapping_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_mapping_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicies_to_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicies_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf_session_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession_creator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf_session_args\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_fragment_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrollout_fragment_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount_steps_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount_steps_by\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhorizon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessor_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessor_pref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_async\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompress_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompress_observations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_envs_per_worker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_rewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalize_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecreated_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecreated_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_worker_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote_worker_envs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_env_batch_wait_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote_env_batch_wait_ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoft_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoft_horizon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_done_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_done_at_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfake_sampler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_python_environs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_python_environs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_env_checking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisable_env_checking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py:567\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, log_dir, log_level, callbacks, input_creator, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, disable_env_checking)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_sub_env_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_make_sub_env_fn(\n\u001b[1;32m    562\u001b[0m         env_creator, env_context, validate_env, wrap, seed\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspaces \u001b[38;5;241m=\u001b[39m spaces\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_dict \u001b[38;5;241m=\u001b[39m \u001b[43m_determine_spaces_for_multi_agent_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_config\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;66;03m# Set of IDs of those policies, which should be trained. This property\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# is optional and mainly used for backward compatibility.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicies_to_train \u001b[38;5;241m=\u001b[39m policies_to_train\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py:2121\u001b[0m, in \u001b[0;36m_determine_spaces_for_multi_agent_dict\u001b[0;34m(multi_agent_policies_dict, env, spaces, policy_config)\u001b[0m\n\u001b[1;32m   2119\u001b[0m         obs_space \u001b[38;5;241m=\u001b[39m policy_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2120\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`observation_space` not provided in PolicySpec for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and env does not have an observation space OR \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2124\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno spaces received from other workers\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m env(s) OR no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`observation_space` specified in config!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2128\u001b[0m     multi_agent_policies_dict[pid]\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m obs_space\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m policy_spec\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: `observation_space` not provided in PolicySpec for default_policy and env does not have an observation space OR no spaces received from other workers' env(s) OR no `observation_space` specified in config!"
     ]
    }
   ],
   "source": [
    "DQN(config={\"framework\":\"torch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6607b0aa-b299-4dd3-82e7-62c47ffbb009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [62], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_env\n\u001b[1;32m      4\u001b[0m register_env(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_env\u001b[39m\u001b[38;5;124m\"\u001b[39m, OTEnv)\n\u001b[0;32m----> 5\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframework\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:414\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     }\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py:524\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;66;03m# has been deprecated.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:185\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    182\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_worker:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_worker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRolloutWorker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_policy_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Initially, policy_specs will be inferred from config dict.\u001b[39;49;00m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py:892\u001b[0m, in \u001b[0;36mWorkerSet._make_worker\u001b[0;34m(self, cls, env_creator, validate_env, policy_cls, policy_specs, worker_index, num_workers, recreated_worker, config, spaces)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     extra_python_environs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_python_environs_for_worker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 892\u001b[0m worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_mapping_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicy_mapping_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicies_to_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolicies_to_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf_session_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession_creator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf_session_args\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrollout_fragment_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrollout_fragment_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount_steps_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount_steps_by\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhorizon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessor_pref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreprocessor_pref\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_async\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_async\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompress_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompress_observations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_envs_per_worker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiagent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_rewards\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalize_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_actions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecreated_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecreated_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_worker_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote_worker_envs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_env_batch_wait_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mremote_env_batch_wait_ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoft_horizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoft_horizon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_done_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_done_at_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mworker_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfake_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfake_sampler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_python_environs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_python_environs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_env_checking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisable_env_checking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m worker\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py:492\u001b[0m, in \u001b[0;36mRolloutWorker.__init__\u001b[0;34m(self, env_creator, validate_env, policy_spec, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, count_steps_by, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, clip_rewards, normalize_actions, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, recreated_worker, log_dir, log_level, callbacks, input_creator, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler, spaces, policy, disable_env_checking)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Create a (single) env for this worker.\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    487\u001b[0m     worker_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m num_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m policy_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# Run the `env_creator` function passing the EnvContext.\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_context\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# Validate environment (general validation function).\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_env_checking:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "register_env(\"my_env\", OTEnv)\n",
    "algo = DQN(env=\"my_env\",config={\"framework\":\"torch\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59ed2bdd-a15b-4ba4-bc86-2e7e5ff3a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(\"OT-v0\", lambda config: gym_OT(\"OT-v0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d551ff4b-fd90-4a2e-8aef-a26f7703c2d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Second argument must be callable.', <module 'gym_OT' from '/usr/local/lib/python3.8/dist-packages/gym_OT/__init__.py'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ppo\n\u001b[0;32m----> 5\u001b[0m \u001b[43mregister_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgym_OT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/ray/tune/registry.py:117\u001b[0m, in \u001b[0;36mregister_env\u001b[0;34m(name, env_creator)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"Register a custom environment for use with RLlib.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03mThis enables the environment to be accessed on every Ray process\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    env_creator: Callable that creates an env.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(env_creator):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecond argument must be callable.\u001b[39m\u001b[38;5;124m\"\u001b[39m, env_creator)\n\u001b[1;32m    118\u001b[0m _global_registry\u001b[38;5;241m.\u001b[39mregister(ENV_CREATOR, name, env_creator)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Second argument must be callable.', <module 'gym_OT' from '/usr/local/lib/python3.8/dist-packages/gym_OT/__init__.py'>)"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "\n",
    "register_env(\"my_env\", gym_OT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2eed701-f294-477a-baf9-04c837f0a480",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-17 20:48:49,880\tWARNING utils.py:604 -- Ray currently does not support initializing Raywith fractional cpus. Your num_cpus will be truncated from 5.985 to 5.\n",
      "2022-11-17 20:48:50,085\tINFO worker.py:1528 -- Started a local Ray instance.\n",
      "== Status ==\n",
      "Current time: 2022-11-17 20:48:56 (running for 00:00:04.79)\n",
      "Memory usage on this node: 4.0/29.4 GiB \n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1.0/5 CPUs, 0/1 GPUs, 0.0/14.89 GiB heap, 0.0/7.45 GiB objects (0.0/1.0 accelerator_type:P5000)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "2022-11-17 20:48:56,352\tERROR trial_runner.py:993 -- Trial DQN_OT-v0_3e06f_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 2291, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=10467, ip=10.42.32.217, repr=DQN)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 676, in make\n",
      "    return registry.make(id, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 490, in make\n",
      "    versions = self.env_specs.versions(namespace, name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 220, in versions\n",
      "    self._assert_name_exists(namespace, name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 297, in _assert_name_exists\n",
      "    raise error.NameNotFound(message)\n",
      "gym.error.NameNotFound: Environment `OT` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::DQN.__init__()\u001b[39m (pid=10467, ip=10.42.32.217, repr=DQN)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 524, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py\", line 185, in __init__\n",
      "    self._local_worker = self._make_worker(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py\", line 892, in _make_worker\n",
      "    worker = cls(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('OT-v0') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For VizDoom support: Install VizDoom\n",
      "   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "   `pip install vizdoomgym`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\n",
      "== Status ==\n",
      "Current time: 2022-11-17 20:48:56 (running for 00:00:04.80)\n",
      "Memory usage on this node: 4.0/29.4 GiB \n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/5 CPUs, 0/1 GPUs, 0.0/14.89 GiB heap, 0.0/7.45 GiB objects (0.0/1.0 accelerator_type:P5000)\n",
      "Result logdir: /root/ray_results/default\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "Number of errored trials: 1\n",
      "+-----------------------+--------------+---------------------------------------------------------------------------------+\n",
      "| Trial name            |   # failures | error file                                                                      |\n",
      "|-----------------------+--------------+---------------------------------------------------------------------------------|\n",
      "| DQN_OT-v0_3e06f_00000 |            1 | /root/ray_results/default/DQN_OT-v0_3e06f_00000_0_2022-11-17_20-48-51/error.txt |\n",
      "+-----------------------+--------------+---------------------------------------------------------------------------------+\n",
      "\n",
      "2022-11-17 20:48:56,359\tERROR ray_trial_executor.py:111 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 102, in _post_stop_cleanup\n",
      "    ray.get(future, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 2291, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=10467, ip=10.42.32.217, repr=DQN)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 676, in make\n",
      "    return registry.make(id, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 490, in make\n",
      "    versions = self.env_specs.versions(namespace, name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 220, in versions\n",
      "    self._assert_name_exists(namespace, name)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 297, in _assert_name_exists\n",
      "    raise error.NameNotFound(message)\n",
      "gym.error.NameNotFound: Environment `OT` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::DQN.__init__()\u001b[39m (pid=10467, ip=10.42.32.217, repr=DQN)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 524, in setup\n",
      "    self.workers = WorkerSet(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py\", line 185, in __init__\n",
      "    self._local_worker = self._make_worker(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py\", line 892, in _make_worker\n",
      "    worker = cls(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('OT-v0') is:\n",
      "a) Not a supported/installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For VizDoom support: Install VizDoom\n",
      "   (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "   `pip install vizdoomgym`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config['env'] = [name]`.\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m 2022-11-17 20:48:56,333\tINFO simple_q.py:307 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m 2022-11-17 20:48:56,336\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m 2022-11-17 20:48:56,344\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=10467, ip=10.42.32.217, repr=DQN)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 676, in make\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     return registry.make(id, **kwargs)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 490, in make\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     versions = self.env_specs.versions(namespace, name)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 220, in versions\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     self._assert_name_exists(namespace, name)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\", line 297, in _assert_name_exists\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     raise error.NameNotFound(message)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m gym.error.NameNotFound: Environment `OT` doesn't exist.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m \u001b[36mray::DQN.__init__()\u001b[39m (pid=10467, ip=10.42.32.217, repr=DQN)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 524, in setup\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     self.workers = WorkerSet(\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py\", line 185, in __init__\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     self._local_worker = self._make_worker(\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/worker_set.py\", line 892, in _make_worker\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     worker = cls(\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in __init__\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/env/utils.py\", line 52, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('OT-v0') is:\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m a) Not a supported/installed environment.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m \n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m Try one of the following:\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    For VizDoom support: Install VizDoom\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    (https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md) and\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    `pip install vizdoomgym`.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    tune.register('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    Then in your config, do `config['env'] = [name]`.\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[2m\u001b[36m(DQN pid=10467)\u001b[0m    `ray.rllib.examples.env.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/rllib\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/scripts.py\", line 41, in cli\n",
      "    train.run(options, train_parser)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/train.py\", line 271, in run\n",
      "    run_experiments(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/tune.py\", line 867, in run_experiments\n",
      "    return run(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/tune.py\", line 771, in run\n",
      "    raise TuneError(\"Trials did not complete\", incomplete_trials)\n",
      "ray.tune.error.TuneError: ('Trials did not complete', [DQN_OT-v0_3e06f_00000])\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!rllib train --run DQN --env OT-v0 --framework torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dcec370-c7db-4f89-aa57-4310eab689c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14127)\u001b[0m 2022-11-17 18:39:32,920\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2022-11-17 18:39:33,559\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14319)\u001b[0m 2022-11-17 18:39:37,491\tWARNING env.py:159 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2022-11-17 18:39:37,592\tINFO trainable.py:164 -- Trainable.setup took 10.994 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from ray.rllib.algorithms.dqn import DQN\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    \"num_workers\": 7,\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_gpus\" : 1,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [640, 640,300],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_interval\":1,\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "algo = DQN(config=config)\n",
    "\n",
    "# for it in tqdm(range(30)):\n",
    "#     algo.train()\n",
    "#     print(it,'  ',algo.evaluate()['evaluation']['episode_reward_mean'])\n",
    "\n",
    "\n",
    "# algo.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ece971e-af9b-4764-8f85-650d1f717c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ray.tune.tune_config.TuneConfig"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.TuneConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e319c9d-667f-49bf-ba01-51f6856698af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mujoco-py<2.2,>=2.1\n",
      "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fasteners~=0.15\n",
      "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from mujoco-py<2.2,>=2.1) (1.23.4)\n",
      "Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.8/dist-packages (from mujoco-py<2.2,>=2.1) (2.22.4)\n",
      "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.8/dist-packages (from mujoco-py<2.2,>=2.1) (1.15.1)\n",
      "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.8/dist-packages (from mujoco-py<2.2,>=2.1) (0.29.32)\n",
      "Collecting glfw>=1.4.0\n",
      "  Downloading glfw-2.5.5-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1) (2.21)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.8/dist-packages (from imageio>=2.1.2->mujoco-py<2.2,>=2.1) (9.2.0)\n",
      "Installing collected packages: glfw, fasteners, mujoco-py\n",
      "Successfully installed fasteners-0.18 glfw-2.5.5 mujoco-py-2.1.2.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -U 'mujoco-py<2.2,>=2.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7906e2de-845f-4a53-8a2a-674063d1379b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 17:25:21,548\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-11-17 17:25:28</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:05.09        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.1/29.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/15.55 GiB heap, 0.0/7.78 GiB objects (0.0/1.0 accelerator_type:P5000)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                 </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_Taxi-v3_d1bc5_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/DQN/DQN_Taxi-v3_d1bc5_00000_0_num_sgd_iter=20,sgd_minibatch_size=512,train_batch_size=40000_2022-11-17_17-25-23/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  lambda</th><th style=\"text-align: right;\">  num_sgd_iter</th><th style=\"text-align: right;\">  sgd_minibatch_size</th><th style=\"text-align: right;\">  train_batch_size</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_Taxi-v3_d1bc5_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.907474</td><td style=\"text-align: right;\">            20</td><td style=\"text-align: right;\">                 512</td><td style=\"text-align: right;\">             40000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 17:25:28,896\tERROR trial_runner.py:993 -- Trial DQN_Taxi-v3_d1bc5_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/execution/ray_trial_executor.py\", line 1050, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 2291, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=7783, ip=10.42.32.217, repr=DQN)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
      "    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 439, in setup\n",
      "    self.config = self.merge_trainer_configs(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 2228, in merge_trainer_configs\n",
      "    return deep_update(\n",
      "Exception: Unknown config parameter `clip_param`\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_Taxi-v3_d1bc5_00000</td><td>d1bc5_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m 2022-11-17 17:25:28,873\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=7783, ip=10.42.32.217, repr=DQN)\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 414, in __init__\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 161, in __init__\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 439, in setup\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m     self.config = self.merge_trainer_configs(\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 2228, in merge_trainer_configs\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m     return deep_update(\n",
      "\u001b[2m\u001b[36m(DQN pid=7783)\u001b[0m Exception: Unknown config parameter `clip_param`\n",
      "2022-11-17 17:25:29,014\tERROR tune.py:773 -- Trials did not complete: [DQN_Taxi-v3_d1bc5_00000]\n",
      "2022-11-17 17:25:29,016\tINFO tune.py:777 -- Total run time: 5.21 seconds (5.09 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters:  None\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True,num_cpus=8,num_gpus=1)\n",
    "\n",
    "import random\n",
    "\n",
    "from ray import air, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Postprocess the perturbed config to ensure it's still valid\n",
    "    def explore(config):\n",
    "        # ensure we collect enough timesteps to do sgd\n",
    "        if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n",
    "            config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n",
    "        # ensure we run at least one sgd iter\n",
    "        if config[\"num_sgd_iter\"] < 1:\n",
    "            config[\"num_sgd_iter\"] = 1\n",
    "        return config\n",
    "\n",
    "    pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        perturbation_interval=120,\n",
    "        resample_probability=0.25,\n",
    "        \n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations={\n",
    "            \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "            # \"clip_param\": lambda: random.uniform(0.01, 0.5),\n",
    "            \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"num_sgd_iter\": lambda: random.randint(1, 30),\n",
    "            \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "            \"train_batch_size\": lambda: random.randint(2000, 160000),\n",
    "        },\n",
    "        custom_explore_fn=explore,\n",
    "    )\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        \"DQN\",\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"episode_reward_mean\",\n",
    "            mode=\"max\",\n",
    "            scheduler=pbt,\n",
    "            num_samples=1,\n",
    "            reuse_actors=True,\n",
    "            \n",
    "            \n",
    "        ),\n",
    "        \n",
    "        param_space={\n",
    "            \"env\": \"Taxi-v3\",\n",
    "            \"framework\": \"torch\",\n",
    "            \"num_workers\": 7,\n",
    "            \"num_gpus\": 1, # number of GPUs to use\n",
    "            \"model\": {\"free_log_std\": True,\n",
    "                      \"fcnet_hiddens\": [64, 64],\n",
    "                      \"fcnet_activation\": \"relu\",\n",
    "                     },\n",
    "            # These params are tuned from a fixed starting value.\n",
    "            # \"lambda\": 0.95,\n",
    "            \"clip_param\": 0.2,\n",
    "            \"lr\": 1e-4,\n",
    "            # These params start off randomly drawn from a set.\n",
    "            \"num_sgd_iter\": tune.choice([10, 20, 30]),\n",
    "            \"sgd_minibatch_size\": tune.choice([128, 512, 2048]),\n",
    "            \"train_batch_size\": tune.choice([10000, 20000, 40000]),\n",
    "            \n",
    "        },\n",
    "        \n",
    "    )\n",
    "    results = tuner.fit()\n",
    "\n",
    "    print(\"best hyperparameters: \", results.get_best_result().config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f00739-d44d-4542-873d-e1fb2470574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = algo.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45efa2d8-6b8d-4be8-b806-03e5196f396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-532.1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59150f0-31f2-47ba-93b4-9d13c4c6bad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
